# AWS Data Analysis Pipeline (S3, Glue, Athena)

This project demonstrates a serverless data analysis pipeline built using core AWS services. The objective is to ingest a raw CSV dataset (`Amazon Sale Report.csv`), use AWS Glue to catalog the data, and then use Amazon Athena to run complex analytical SQL queries on the data directly from S3.

The entire workflow is **serverless**, meaning we don't need to manage any underlying infrastructure, and we only pay for the storage and query-time we use.

## Project Workflow

1.  **Storage (S3):** A raw data S3 bucket is created to store the `Amazon Sale Report.csv` file. A second bucket is created to store the results of our Athena queries.
2.  **Permissions (IAM):** An IAM Role is created to give the AWS Glue service permission to access the S3 buckets.
3.  **Cataloging (Glue):** An AWS Glue Crawler is configured to scan the raw data bucket. It automatically infers the data schema (columns and data types) and creates a table in the AWS Glue Data Catalog.
4.  **Querying (Athena):** Amazon Athena is used to query the table created by Glue. Athena uses standard SQL to query the CSV file "in-place" directly from S3.
5.  **Monitoring (CloudWatch):** Amazon CloudWatch is used to monitor the logs generated by the Glue Crawler to ensure it ran successfully or to debug any errors.

---

## AWS Services Explained

### 1. AWS S3 (Simple Storage Service)

* **What it is:** S3 is a highly durable and scalable object storage service. You can think of it as a limitless hard drive in the cloud, designed to store any amount of data.
* **Role in this Project:**
    * **Data Lake:** S3 serves as our data lake, holding the raw `Amazon Sale Report.csv` file.
    * **Query Results:** Athena is configured to save all query results as new CSV files in a separate S3 bucket.

*(Your S3 bucket configuration)*
![My S3 Buckets](AWS_S3.png)

### 2. AWS IAM (Identity and Access Management)

* **What it is:** IAM is the security service that manages *who* (users, services) can do *what* (permissions, actions) in your AWS account.
* **Role in this Project:**
    * We created an **IAM Role** (a set of permissions) that our Glue service could "assume."
    * This role was granted permissions (policies) to allow it to read data from our S3 bucket (`AmazonS3FullAccess`) and to perform Glue operations (`AWSGlueConsoleFullAccess`). Without this role, the Glue Crawler would be denied access to your S3 data.

*(Your IAM role configuration)*
![My IAM Role](AWS_IAM.png)

### 3. AWS Glue

* **What it is:** AWS Glue is a serverless data integration service. Its primary components are a **Crawler** and a **Data Catalog**.
* **Role in this Project:**
    * **Glue Crawler:** We used the crawler to scan our S3 bucket. It automatically analyzed the `Amazon Sale Report.csv` file, identified all the columns (like `Order ID`, `Date`, `Status`, `Amount`), and inferred their data types (string, number, etc.).
    * **Glue Data Catalog:** The crawler then registered this schema as a table (e.t., `raw`) inside a database (e.g., `handson_output_db`). This catalog acts as a central "metastore" or "phone book" that tells other services (like Athena) where your data lives and what it looks like.

*(Your Glue database configuration)*
![My Glue Database](AWS_Glue.png)

### 4. Amazon Athena

* **What it is:** Amazon Athena is a serverless, interactive query service that makes it easy to analyze data in S3 using standard SQL.
* **Role in this Project:**
    * This is our primary analysis tool.
    * Athena connects directly to the Glue Data Catalog, sees the `raw` table, and allows us to query it as if it were a traditional database.
    * We can run complex SQL queries with joins, window functions, and aggregations on the CSV file without ever loading it into a database.

### 5. Amazon CloudWatch

* **What it is:** CloudWatch is the monitoring and logging service for AWS. It collects logs, metrics, and events from all your AWS resources.
* **Role in this Project:**
    * When our Glue Crawler ran, it published its logs to CloudWatch.
    * We used the CloudWatch logs to verify that the crawler ran successfully and to troubleshoot any potential errors (e.g., permission issues).

*(You will need to add your own screenshot of the CloudWatch logs here, as instructed in the PDF.)*
`[Add your CloudWatch screenshot here]`

---

## SQL Queries for Analysis

Here are the adapted SQL queries for the project, based on the columns available in the `Amazon Sale Report.csv` file.

*Note: All "profit"-based queries were adapted to analyze `Amount` (sales) or `Qty` (quantity), as `profit` was not an available column.*

### 1. Cumulative Sales Over Time (for 2022)

```sql
-- This query calculates the running total of sales for each day of 2022.
-- It assumes the date format is 'MM-DD-YY'
WITH DailySales AS (
    SELECT
        date_parse("Date", '%m-%d-%y') AS sale_date,
        SUM(CAST("Amount" AS DOUBLE)) AS daily_sales
    FROM
        "AwsDataCatalog"."handson_output_db"."raw"
    WHERE
        "Status" IN ('Shipped', 'Shipped - Delivered to Buyer')
        AND substr("Date", 7, 2) = '22'
    GROUP BY
        date_parse("Date", '%m-%d-%y')
)
SELECT
    sale_date,
    daily_sales,
    SUM(daily_sales) OVER (
        ORDER BY sale_date ASC
        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
    ) AS cumulative_sales
FROM
    DailySales
ORDER BY
    sale_date ASC;
